{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Banco de Dados Dimensional Projeto pratico em modo de desafio com a criacao de um banco de dados dimensional desenvolvido como atividade avaliativa do Bootcamp Engenheiro de Dados da XPEducacao. Estrutura do Projeto \u251c\u2500 dags: pasta de dags do Apache Airflow \u251c\u2500 data: pasta auxiliar para armazenamento de dados durante a execucao das dags \u251c\u2500 docs: pasta de documentacao do mkdocs \u2502 \u2514\u2500 media \u2502 \u2514\u2500 index.md: pagina inicial da documentacao do projeto \u251c\u2500 logs: pasta de logs do Apache Airflow \u251c\u2500 notebooks: jupyter notebook com a analise exploratoria dos dados \u251c\u2500 plugins: plugin do Apache Airflow \u251c\u2500 mkdocs.yaml: arquivo de configuracao do mkdocs \u251c\u2500 README.md \u251c\u2500 requirements.txt: bibliotecas requeridas para rodar o projeto \u251c\u2500 docker-compose: arquivo para realizar o build dos containers docker do projeto Objetivo do Projeto O modelo dimensional foi criado a partir das metricas propostas pelo desafio . Visando tornar o projeto mais robusto, criou-se um pipeline de dados orquestrado pelo Apache Airflow que recupera os dados em formato .csv , realiza os procedimentos de transformacao e compatibilizacao dos dados com o modelo dimensional desenhado. Os dados modelados sao colocados em um bucket da AWS S3 e, por fim, os dados sao armazenados em um cluster na AWS Redshift . Os dados do Redshift sao consumidos pelo Power BI , onde sera criado um dashboard de acordo com as metricas e necessidades de negocio levantadas. Levantamento de Requisitos Empresa de Seguros de Automoveis Cada cliente possui CPF, nome, sexo, endere\u00e7o e telefones de contato (celular e fixo), mas podem vir a ter outros telefones no futuro. Os carros possuem uma placa, marca, modelo, ano, chassi e cor. Cada carro tem determinado n\u00famero de sinistros de acidentes associados a ele, sabendo que pode ter ocorrido m\u00faltiplos acidentes ou nenhum. J\u00e1 os sinistros devem ser identificados por um c\u00f3digo \u00fanico, data de ocorr\u00eancia, hora de ocorr\u00eancia, local de ocorr\u00eancia (Endere\u00e7o completo) e condutor, que pode ou n\u00e3o ser o titular da ap\u00f3lice e precisa ser devidamente registrado no sistema. Al\u00e9m disso, o sinistro possui um tipo de ocorr\u00eancia (Colis\u00e3o, Roubo ou Furto, Inc\u00eandio, Enchente, Retrovisor, dentre outros). Um cliente ou pessoa pode ter v\u00e1rias ap\u00f3lices (m\u00ednimo uma) vigentes ou nenhuma, e cada ap\u00f3lice de seguro tem um identificador \u00fanico e s\u00f3 pertence a um cliente e a somente um carro, mas pode ter v\u00e1rios condutores vinculados, e tem data de in\u00edcio e fim da vig\u00eancia, valor total assegurado e valor franquia associados a ela. \u00c9 importante saber que o carro pode ter v\u00e1rias ap\u00f3lices vinculadas a ele, mas apenas uma vigente. E sempre que houver um sinistro \u00e9 preciso saber qual o carro envolvido, al\u00e9m do n\u00famero da ap\u00f3lice, uma vez que este precisa ser assegurado por uma ap\u00f3lice. Mesmo que o atendimento seja para terceiros vai ser registrado no sinistro do carro vinculado \u00e0 ap\u00f3lice da empresa em quest\u00e3o. Levantamento de Metricas O quadro abaixo contem todas as metricas que irao responder as questoes de negocio definidas pela empresa. Bus Matrix O quadro abaixo estabelece uma definicao dos grupos de dimensoes e fatos que devem estar no modelo de dados do projeto. Modelo Multidimensional do Data Mart Stack Tecnologica Apache Airflow 2.0 AWS Redshift AWS S3 Docker 20.10.17 Python 3.10.7 Power BI Decisoes Arquiteturais do Projeto O Apache Airflow e o orquestrador de pipelines mais utilizado do mercado, personalizavel e permite a escalabilidade do projeto com a possibilidade de processamento distribuido. O AWS Redshift foi escolhido por ser uma solucao totalmente gerenciada e permitir consumir os dados diretamento do AWS S3 . Alem disso, o banco e otimizado para solucoes de DW. O AWS S3 e muito utilizado no mercado pela sua flexibilidade e a possibilidade de consumo dos dados em seu formato nativo. Alem disso e possivel separar o projeto em uma estrutura de pastas que auxilia o armazenamentos dos dados em todos os estagios do pipeline. O Docker permite uma maior compatibilidade e flexibilidade no ambiente de execucao do projeto. Alem disso pode ser implementado nao apenas em servidores como tambem em solucoes serverless disponiveis em todas as plataformas de cloud. O Power BI foi escolhido por ser uma das solucoes de dataviz mais utilizadas pelo mercado, ser capaz de lidar com grandes quantidades de dados e possuir diversos recursos para analise de dados. Pipeline de Dados Apache Airflow 1. Task docker-operator-etl: task responsavel por executar todas as transformacoes nos dados de acordo com o modelo dimensional pre-definido. A task foi isolada em um container docker para facilitar a compatibilidades com as dependencias necessarias para execucao. Os detalhes das transformacoes podem ser encontrados aqui: julioszeferino/docker-operator-etl . 2. Task envia-arquivos-s3: task responsavel por armazenar os aarquivos no bucket criado no s3. 3. Task cria-tabelas-redshift: task responsavel por estruturar o data warehouse no redshift. 4. Task carrega-dados-redshift: task responsavel por recuperar os dados do s3 e armazenar no cluster redshift do DW. Como Executar este Projeto Crie uma instancia no AWS EC2 ou outro provedor da sua escolha, realize o download deste repositorio e execute o docker-compose para realizar o build dos containers: docker-compose up -d Criar um bucket no AWS S3 com o nome etl-seguros . No painel web do Apache Airflow cadastrar a variavel aws_default_secret com os dados de acesso da sua conta aws: { \"aws_access_key_id\": \"\", \"aws_secret_access_key\" } Crie um database chamado seguros dentro de cluster na AWS Redshift . No painel web do Apache Airflow cadastrar a variavel aws_redshift_seguros_secret com os dados de acesso ao banco: { \"user\": \"\", \"password\": \"\" } Realizar o pull da imagem docker responsavel por realizar o processo de ETL nos dados $ docker pull julioszeferino/docker-operator-etl Executar o pipeline de dados no Apache Airflow Configurar e criar as visualizacoes no Power BI Configuracao das Variaveis no Apache Airflow A configuracao das variaveis de acesso a aws e ao redshift devem ser feitas na interface web do Apache Airflow que por padrao estara disponivel no endereco http://localhost:8080 . Acesse a aba Admin > Variables : Dashboard","title":"Banco de Dados Dimensional"},{"location":"#banco-de-dados-dimensional","text":"Projeto pratico em modo de desafio com a criacao de um banco de dados dimensional desenvolvido como atividade avaliativa do Bootcamp Engenheiro de Dados da XPEducacao.","title":"Banco de Dados Dimensional"},{"location":"#estrutura-do-projeto","text":"\u251c\u2500 dags: pasta de dags do Apache Airflow \u251c\u2500 data: pasta auxiliar para armazenamento de dados durante a execucao das dags \u251c\u2500 docs: pasta de documentacao do mkdocs \u2502 \u2514\u2500 media \u2502 \u2514\u2500 index.md: pagina inicial da documentacao do projeto \u251c\u2500 logs: pasta de logs do Apache Airflow \u251c\u2500 notebooks: jupyter notebook com a analise exploratoria dos dados \u251c\u2500 plugins: plugin do Apache Airflow \u251c\u2500 mkdocs.yaml: arquivo de configuracao do mkdocs \u251c\u2500 README.md \u251c\u2500 requirements.txt: bibliotecas requeridas para rodar o projeto \u251c\u2500 docker-compose: arquivo para realizar o build dos containers docker do projeto","title":"Estrutura do Projeto"},{"location":"#objetivo-do-projeto","text":"O modelo dimensional foi criado a partir das metricas propostas pelo desafio . Visando tornar o projeto mais robusto, criou-se um pipeline de dados orquestrado pelo Apache Airflow que recupera os dados em formato .csv , realiza os procedimentos de transformacao e compatibilizacao dos dados com o modelo dimensional desenhado. Os dados modelados sao colocados em um bucket da AWS S3 e, por fim, os dados sao armazenados em um cluster na AWS Redshift . Os dados do Redshift sao consumidos pelo Power BI , onde sera criado um dashboard de acordo com as metricas e necessidades de negocio levantadas.","title":"Objetivo do Projeto"},{"location":"#levantamento-de-requisitos","text":"","title":"Levantamento de Requisitos"},{"location":"#empresa-de-seguros-de-automoveis","text":"Cada cliente possui CPF, nome, sexo, endere\u00e7o e telefones de contato (celular e fixo), mas podem vir a ter outros telefones no futuro. Os carros possuem uma placa, marca, modelo, ano, chassi e cor. Cada carro tem determinado n\u00famero de sinistros de acidentes associados a ele, sabendo que pode ter ocorrido m\u00faltiplos acidentes ou nenhum. J\u00e1 os sinistros devem ser identificados por um c\u00f3digo \u00fanico, data de ocorr\u00eancia, hora de ocorr\u00eancia, local de ocorr\u00eancia (Endere\u00e7o completo) e condutor, que pode ou n\u00e3o ser o titular da ap\u00f3lice e precisa ser devidamente registrado no sistema. Al\u00e9m disso, o sinistro possui um tipo de ocorr\u00eancia (Colis\u00e3o, Roubo ou Furto, Inc\u00eandio, Enchente, Retrovisor, dentre outros). Um cliente ou pessoa pode ter v\u00e1rias ap\u00f3lices (m\u00ednimo uma) vigentes ou nenhuma, e cada ap\u00f3lice de seguro tem um identificador \u00fanico e s\u00f3 pertence a um cliente e a somente um carro, mas pode ter v\u00e1rios condutores vinculados, e tem data de in\u00edcio e fim da vig\u00eancia, valor total assegurado e valor franquia associados a ela. \u00c9 importante saber que o carro pode ter v\u00e1rias ap\u00f3lices vinculadas a ele, mas apenas uma vigente. E sempre que houver um sinistro \u00e9 preciso saber qual o carro envolvido, al\u00e9m do n\u00famero da ap\u00f3lice, uma vez que este precisa ser assegurado por uma ap\u00f3lice. Mesmo que o atendimento seja para terceiros vai ser registrado no sinistro do carro vinculado \u00e0 ap\u00f3lice da empresa em quest\u00e3o.","title":"Empresa de Seguros de Automoveis"},{"location":"#levantamento-de-metricas","text":"O quadro abaixo contem todas as metricas que irao responder as questoes de negocio definidas pela empresa.","title":"Levantamento de Metricas"},{"location":"#bus-matrix","text":"O quadro abaixo estabelece uma definicao dos grupos de dimensoes e fatos que devem estar no modelo de dados do projeto.","title":"Bus Matrix"},{"location":"#modelo-multidimensional-do-data-mart","text":"","title":"Modelo Multidimensional do Data Mart"},{"location":"#stack-tecnologica","text":"Apache Airflow 2.0 AWS Redshift AWS S3 Docker 20.10.17 Python 3.10.7 Power BI","title":"Stack Tecnologica"},{"location":"#decisoes-arquiteturais-do-projeto","text":"O Apache Airflow e o orquestrador de pipelines mais utilizado do mercado, personalizavel e permite a escalabilidade do projeto com a possibilidade de processamento distribuido. O AWS Redshift foi escolhido por ser uma solucao totalmente gerenciada e permitir consumir os dados diretamento do AWS S3 . Alem disso, o banco e otimizado para solucoes de DW. O AWS S3 e muito utilizado no mercado pela sua flexibilidade e a possibilidade de consumo dos dados em seu formato nativo. Alem disso e possivel separar o projeto em uma estrutura de pastas que auxilia o armazenamentos dos dados em todos os estagios do pipeline. O Docker permite uma maior compatibilidade e flexibilidade no ambiente de execucao do projeto. Alem disso pode ser implementado nao apenas em servidores como tambem em solucoes serverless disponiveis em todas as plataformas de cloud. O Power BI foi escolhido por ser uma das solucoes de dataviz mais utilizadas pelo mercado, ser capaz de lidar com grandes quantidades de dados e possuir diversos recursos para analise de dados.","title":"Decisoes Arquiteturais do Projeto"},{"location":"#pipeline-de-dados-apache-airflow","text":"1. Task docker-operator-etl: task responsavel por executar todas as transformacoes nos dados de acordo com o modelo dimensional pre-definido. A task foi isolada em um container docker para facilitar a compatibilidades com as dependencias necessarias para execucao. Os detalhes das transformacoes podem ser encontrados aqui: julioszeferino/docker-operator-etl . 2. Task envia-arquivos-s3: task responsavel por armazenar os aarquivos no bucket criado no s3. 3. Task cria-tabelas-redshift: task responsavel por estruturar o data warehouse no redshift. 4. Task carrega-dados-redshift: task responsavel por recuperar os dados do s3 e armazenar no cluster redshift do DW.","title":"Pipeline de Dados Apache Airflow"},{"location":"#como-executar-este-projeto","text":"Crie uma instancia no AWS EC2 ou outro provedor da sua escolha, realize o download deste repositorio e execute o docker-compose para realizar o build dos containers: docker-compose up -d Criar um bucket no AWS S3 com o nome etl-seguros . No painel web do Apache Airflow cadastrar a variavel aws_default_secret com os dados de acesso da sua conta aws: { \"aws_access_key_id\": \"\", \"aws_secret_access_key\" } Crie um database chamado seguros dentro de cluster na AWS Redshift . No painel web do Apache Airflow cadastrar a variavel aws_redshift_seguros_secret com os dados de acesso ao banco: { \"user\": \"\", \"password\": \"\" } Realizar o pull da imagem docker responsavel por realizar o processo de ETL nos dados $ docker pull julioszeferino/docker-operator-etl Executar o pipeline de dados no Apache Airflow Configurar e criar as visualizacoes no Power BI","title":"Como Executar este Projeto"},{"location":"#configuracao-das-variaveis-no-apache-airflow","text":"A configuracao das variaveis de acesso a aws e ao redshift devem ser feitas na interface web do Apache Airflow que por padrao estara disponivel no endereco http://localhost:8080 . Acesse a aba Admin > Variables :","title":"Configuracao das Variaveis no Apache Airflow"},{"location":"#dashboard","text":"","title":"Dashboard"}]}